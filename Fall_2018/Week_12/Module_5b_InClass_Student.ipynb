{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5, Week 2 In Class Exercise\n",
    "\n",
    "Probability distributions and hypothesis testing\n",
    "\n",
    "**Before class reading: Chapters 11.3 from Data8 textbook and Shearer and Stark (2014)**\n",
    "\n",
    "**Last week we:**\n",
    "- Review some key statistics topics: samples versus populations and empirical versus theorectical distributions\n",
    "- Simulate a head/tail coin toss i.e. Binomial distribution\n",
    "- Simulate cars passing a point i.e. Poisson distribution\n",
    "- Simulate geomagnetic polarity reversals i.e. Gamma distribution\n",
    "\n",
    "**Our goals for today:**\n",
    "- Be the unsung heros of the scientific method, the Reproducers!\n",
    "- Reproduce portions of Shearer, P. M., & Stark, P. B. (2012). Global risk of big earthquakes has not recently increased. _Proceedings of the National Academy of Sciences_, 109(3), 717-721.\n",
    "- Compute the probability that there has been a recent increase in the number of large EQ.\n",
    "- Test the hypothesis that main stock earthquakes are Poissonian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "from scipy import stats\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will use is a global catalog of all M≥7.0 earthquake events, downloaded from USGS: https://earthquake.usgs.gov/earthquakes/search/\n",
    "\n",
    "The columns are: time, latitude, longitude, depth, mag, magType, nst, gap, dmin, rms, net, id, updated, place, type, horizontalError, depthError, magError, magNst, status, locationSource, magSource\n",
    "\n",
    "We don't need most of these columns and can drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and cleanup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('query.csv')\n",
    "raw_eq_data=data.drop(['nst','gap','dmin','rms','net','id','type',\n",
    "                       'horizontalError','depthError','magError','magNst','status',\n",
    "                       'locationSource','magSource'], axis=1)\n",
    "raw_eq_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare variables which we will use further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat=raw_eq_data.latitude.values\n",
    "lon=...\n",
    "mag=...\n",
    "dates=..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the string in the 'time' column to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_obj_1 = datetime.datetime.strptime(dates[0], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "date_time_obj_1.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a timeline of event occurrence times in days since first event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nevt = len(dates) # number of raw events\n",
    "days=np.zeros(nevt) # initialize the size of the array days\n",
    "# convert string in time column to datetime objects and \n",
    "# calculate number of days from first event (i.e. make a days numberline)\n",
    "for i in np.arange(0,nevt,1): \n",
    "    date_time_obj = datetime.datetime.strptime(...[i], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    d0 = datetime.date(date_time_obj_1.year, date_time_obj_1.month, date_time_obj_1.day)\n",
    "    d1 = datetime.date(date_time_obj.year, date_time_obj.month, date_time_obj.day)\n",
    "    delta = ... # \n",
    "    days[i]=delta.days # fill days in with the number of days since the first event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the raw catalog days vs. magnitude, and print the number of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Catalog\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(...,...,'o',alpha=0.2,markersize=5)\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Event Catalog')\n",
    "ax.grid()\n",
    "\n",
    "print('Number of events before declustering: %4i' %(nevt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decluster Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll decluster our catalog following Shearer and Stark (2012):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Because our focus here is on the global scale, we adopt the\n",
    "conservative and simple approach of removing events for which\n",
    "preceding larger earthquakes occur within 3 yr and 1,000 km. Declustering\n",
    "in this manner removes many events that might not traditionally\n",
    "be classified as aftershocks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did back in Module 2b, we need a function to compute the great circle distance in km between geographic locations given in Lat,Lon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function computes the spherical earth distance between to geographic points and is used in the\n",
    "#declustering algorithm below\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "\n",
    "    All args must be of equal length.\n",
    "    \n",
    "    The first pair can be singular and the second an array\n",
    "\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) # convert degrees lat, lon to radians\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2  # great circle inside sqrt\n",
    "\n",
    "    c = 2 * np.arcsin(np.sqrt(a))   # great circle angular separation\n",
    "    km = 6371.0 * c   # great circle distance in km, earth radius = 6371.0 km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll decluster the catalog. The Dtest and Ttest are MUCH simpiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decluster the Catalog  Note: This cell may take a few minute to complete\n",
    "cnt=0 # initialize a counting variable\n",
    "save=np.zeros((1,10000000),dtype=int) # initialize a counting variable\n",
    "for i in range(0,nevt,1):   # step through EQ catalog\n",
    "    # definitions of Dtest and Ttest aftershock window bounds\n",
    "    Dtest=...   # distance bounds 1000 km\n",
    "    Ttest=...  # aftershock time bounds 3 years\n",
    "    \n",
    "    a=days[i+1:nevt]-days[i]    # time interval in days to subsequent earthquakes in catalog\n",
    "    m=mag[i+1:nevt]   # magnitudes of subsequent earthquakes in catalog\n",
    "    b=haversine_np(lon[i],lat[i],lon[i+1:nevt],lat[i+1:nevt]) # distance in km to subsequent EQs in catalog\n",
    "    \n",
    "    icnt=np.count_nonzero(a <= Ttest)   # counts the number of potential aftershocks, \n",
    "                                        # the number of intervals <= Ttest bound\n",
    "    if icnt > 0:  # if there are potential aftershocks\n",
    "        itime=np.array(np.nonzero(a <= Ttest)) + (i+1) # indices of potential aftershocks <= Ttest bound\n",
    "        \n",
    "        for j in range(0,icnt,1):   # loops over the aftershocks         \n",
    "            \n",
    "            if b[j] <= Dtest and m[j] < mag[i]: # test if the event is inside the distance window \n",
    "                                                # and that the event is smaller than the current main EQ\n",
    "                save[0][cnt]=itime[0][j]  # index value of the aftershock\n",
    "                cnt += 1 # increment the counting variable\n",
    "\n",
    "                \n",
    "af_ind=np.delete(np.unique(save),0)   # This is an array of indexes that will be used to delete events flagged \n",
    "                                      # as aftershocks    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the aftershock events\n",
    "declustered_datetime =np.delete(...,af_ind)\n",
    "declustered_days=np.delete(...,...)  #The aftershocks are deleted from the days array \n",
    "declustered_mag=np.delete(...,...)    #The aftershocks are deleted from the mag array \n",
    "n_main=len(declustered_days) # number of M≥7.0 main-stock events\n",
    "\n",
    "days_85 = declustered_days[...] # occurance days when there was a M≥8.5 event\n",
    "n_85 = len(...) # number of M≥8.5 events\n",
    "\n",
    "days_8 = declustered_days[...] # occurance days when there was a M≥8.0 event\n",
    "n_8 = len(...) # number of M≥8.0 events\n",
    "\n",
    "days_75 = declustered_days[...] # occurance days when there was a M≥7.5 event\n",
    "n_75 = len(...) # number of M≥7.5 events\n",
    "\n",
    "print('There are %3i M≥8.5 events, %3i M≥8 events, %3i M≥7.5 events, and  %3i M≥7.0 events.'%(n_85, n_8, n_75, n_main))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of declustered events from Shearer and Stark (2012) were: 759 M≥7.0 events, 330 M≥7.5 events, 75 M≥8.0 events, 16 and M≥8.5 events. What are 2 causes for the difference between event counts? Hint 1: look at the date of the last event in our catalog and compare with the year Shearer and Stark's work was published. Hint 2: look at the \"update\" date for our catalog and compare with the year Shearer and Stark's work was published."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude Versus Time\n",
    "\n",
    "Next we'll reconstruct Figure 1 from Shearer and Stark (2012).\n",
    "\n",
    "\"Fig. 1 shows earthquake magnitudes versus time and smoothed\n",
    "yearly rates of M ≥ 8, M ≥ 7.5, and M ≥ 7 activity. As expected,\n",
    "there are many more small earthquakes than large earthquakes,\n",
    "consistent with a Gutenberg-Richter (GR) power law relationship.\n",
    "The GR b value for the declustered catalog is approximately\n",
    "1.3 for M ≥ 7.5 earthquakes. The eye is a poor judge of randomness \n",
    "and tends to find patterns in random sequences. Thus, the simple appearance of clustering should not be\n",
    "considered convincing evidence for nonrandomness.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure1.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (A) Global earthquake magnitudes since 1900 after regional declustering of events.\n",
    "# Plot DeClustered Catalog\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(..., ...,'o',alpha=0.5,markersize=5)\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Declustered Event Catalog')\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print('Number of events after declustering: %4i' %(n_main))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Panels B,C, and D are Yearly rates of M ≥ 8, M ≥ 7.5, and M ≥ 7 earthquakes. Rates are five-year running averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual rates\n",
    "num_years = 119;\n",
    "years = np.arange(1900,2019,1)\n",
    "annual_rate_7=np.zeros(num_years) # initialize the size of the array, and start count at zero\n",
    "annual_rate_75=np.zeros(num_years) \n",
    "annual_rate_8=np.zeros(num_years) \n",
    "for i in np.arange(0,num_years,1):  \n",
    "    for j in np.arange(0,n_main,1):\n",
    "        # parse the year of each event \n",
    "        date_time_obj = datetime.datetime.strptime(declustered_datetime[j], '%Y-%m-%dT%H:%M:%S.%fZ')   \n",
    "        if date_time_obj.year == years[i]:        \n",
    "            ... # count up the events in each year\n",
    "            if declustered_mag[j] >= 7.5:\n",
    "                ... # count up the M≥7.5 events in each year\n",
    "                if declustered_mag[j] >= 8.0:\n",
    "                    ... # count up the M≥ 8 events in each year\n",
    "\n",
    "\n",
    "#  5 year running average rates\n",
    "avg_rate_7=np.zeros(num_years-4) # initialize the size of the array\n",
    "avg_rate_75=np.zeros(num_years-4) \n",
    "avg_rate_8=np.zeros(num_years-4)   \n",
    "# do the running average\n",
    "for i in np.arange(2,num_years-2,1): \n",
    "    avg_rate_7[i-2]=np.mean(...)\n",
    "    avg_rate_75[i-2]=np.mean(...)\n",
    "    avg_rate_8[i-2]=np.mean(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Rate of M>8.0 eq\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(years[2:-2], ...,'-')\n",
    "ax.set(xlabel='Year', ylabel='Rate')\n",
    "plt.text(2000,0.25,'M $\\geqslant$ 8',fontsize=14)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "#Plot Rate of M>7.5 eq\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(..., ...,'-')\n",
    "ax.set(xlabel='Year', ylabel='Rate')\n",
    "plt.text(2000,2.5,'M $\\geqslant$ 7.5',fontsize=14)\n",
    "ax.grid()\n",
    "plt.show() \n",
    "\n",
    "#Plot Rate of M>7.0 eq\n",
    "fig, ax = plt.subplots(figsize=(10,3))\n",
    "ax.plot(..., ...,'-')\n",
    "ax.set(xlabel='Year', ylabel='Rate')\n",
    "plt.text(2000,3,'M $\\geqslant$ 7',fontsize=14)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Nonetheless, past researchers have pointed to several possibly\n",
    "anomalous features that are visible in this plot. First, there were a\n",
    "disproportionate number of very large M ≥ 8.5 earthquakes\n",
    "between 1950 and 1965. Second, there was a dearth of such large\n",
    "earthquakes in the 38 yr from 1966 to 2003. Finally, since 2004\n",
    "there has been an elevated rate of M ≥ 8 earthquakes: The five year\n",
    "running average is at a record high, although there have been\n",
    "rates nearly as high in the past. These anomalies are evident only\n",
    "for the largest earthquakes and are much weaker or absent for\n",
    "smaller earthquakes. This observation implies that if the large\n",
    "earthquake clustering is caused by a physical mechanism, the\n",
    "mechanism must affect M ≥ 8 earthquakes without changing\n",
    "the rate of smaller events. This property is inconsistent with\n",
    "the triggering behavior implied by aftershock sequences, which\n",
    "are observed to have Gutenberg-Richter magnitude-frequency\n",
    "relationships reflecting a preponderance of smaller events.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see these \"anomalous features\" in our catalog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tests\n",
    "\n",
    "Next we'll test how anomalous those features are by simulating random earthquake catalogs!\n",
    "\n",
    "\"How statistically significant are these anomalies for the large\n",
    "earthquakes? Addressing this question is complicated by the fact\n",
    "that virtually every realization of a random process will have features\n",
    "that appear anomalous.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We use Monte Carlo simulations to estimate the probability\n",
    "both of the recent elevated rate of large earthquake activity and\n",
    "of the gap that preceded it, under the null hypothesis that seismicity\n",
    "follows a Poisson process that generates exactly as many\n",
    "events as were in fact observed. That is, under the null hypothesis,\n",
    "the number of events is given and the times of these events are\n",
    "independent, identically distributed (iid) random variables, all\n",
    "with a uniform distribution on the interval in days [0, 40,767].\n",
    "Our estimates are based on 100,000 random catalogs simulated\n",
    "from that joint distribution. The estimated probabilities are the\n",
    "fractions of those 100,000 catalogs that have the apparent anomaly\n",
    "at issue, for instance, the fraction that have at least a given\n",
    "number of events within an interval of a specified length.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.random.uniform(start,end,number)` to draw `n_main` identically distributed (iid) random occurance days between `0` and `declustered_days[-1]` days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Monte Carlo simulation\n",
    "# \"the number of events is given\" \n",
    "# use the same event magnitudes\n",
    "sim_mag=declustered_mag\n",
    "\n",
    "# \"the times of the events are independent, identically distributed (iid) random variables\"\n",
    "# draw random occurance days from a uniform distribution\n",
    "sim_days=np.random.uniform(...,...,...)\n",
    "\n",
    "#Sort events by sim_days\n",
    "ind_d=np.argsort(...)   #determine sort index\n",
    "sim_days=sim_days[ind_d]     #apply sort index\n",
    "sim_mag=sim_mag[ind_d]       #apply sort index\n",
    "\n",
    "#Plot Simulated Catalog\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(..., ...,'o',alpha=0.5,markersize=5)\n",
    "ax.set(xlabel='Days', ylabel='Magnitude',\n",
    "       title='Simulated Event Catalog')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's one simulated catalog, now let's do a statistically significant number. And count the number of simulations that have at least nine of 89 (M ≥ 8) events occuring within 2,269 days of each other or that lack any M ≥ 8.5 events for a period of at least 40 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5000 Monte Carlo simulations\n",
    "num_sims = 5000;\n",
    "cnt_sims_1 = 0 # initialize count at zero\n",
    "cnt_sims_3 = 0\n",
    "for m in np.arange(0,num_sims,1):\n",
    "    sim_mag=declustered_mag\n",
    "    sim_days=np.random.uniform(0,declustered_days[-1],n_main) # draw random occurance days from a uniform distribution\n",
    "    #Sort events\n",
    "    ind_d=np.argsort(sim_days)   #determine sort index\n",
    "    sim_days=sim_days[ind_d]     #apply sort index\n",
    "    sim_mag=sim_mag[ind_d]       #apply sort index\n",
    "    \n",
    "    \n",
    "    # Count the occurances of the anomalous features\n",
    "    \n",
    "    # Test 1: at least nine of 89 (M ≥ 8) events occur within 2,269 days of each other\n",
    "    sim_days_8=sim_days[...] # days with mag>=8.0\n",
    "    for i in np.arange(0,n_8,1): # loop over the M ≥ 8 events\n",
    "        a=... # days to subsequent 8.0 events\n",
    "        n_8_2269_days = len(a[...]) # number of days within 2,269 days of current event\n",
    "        if n_8_2269_days >= 9 : # if there are more than 9 events within 2,269 days count this sim and go to the next\n",
    "            cnt_sims_1 = cnt_sims_1 +1\n",
    "            break\n",
    "        \n",
    "    # Test 2: at least three of 15 (M ≥ 8.5) events occur within 2,266 days of each other\n",
    "    # LEAVE FOR HW\n",
    "    \n",
    "    #Test 3: lack of M ≥ 8.5 events 40 yr gap\n",
    "    sim_days_85=sim_days[sim_mag>=8.5]\n",
    "    for k in np.arange(0,n_85-1,1): # loop over the M ≥ 8.5 events\n",
    "        b=... # days to next 8.5 event\n",
    "        n_85_gap_40year = len(b[...]) # number of 8.5 events with 40 year gap (or more) until next event\n",
    "        if n_85_gap_40year ... : # if there are ANY with 40 year gap until next event count this sim and go to the next\n",
    "            cnt_sims_3 = cnt_sims_3 +1\n",
    "            break\n",
    "        \n",
    "per_chance_1 = 100* cnt_sims_1/num_sims;\n",
    "print('Under the null hypothesis, there is about a ', per_chance_1,'% chance that at least nine of 89 M≥8 events would occur within 2,269 days of each other.')\n",
    "\n",
    "per_chance_3 = 100* cnt_sims_3/num_sims;\n",
    "print('Under the null hypothesis, there is about a ', per_chance_3,'% chance that a 40 year gap between M≥8.5 events would occur.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tests of the Poisson Hypothesis\n",
    "\n",
    "\"A more general question is whether the earthquake catalog, after\n",
    "removing regional clustering, is statistically distinguishable from a\n",
    "realization of a homogenous Poisson process.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1:  KS test that times are iid uniform random variables\n",
    "\n",
    "\"The first test compares the empirical distribution of the times\n",
    "with the uniform distribution. For each magnitude threshold,\n",
    "we determine the times of (locally declustered) events of that\n",
    "magnitude or above. We then perform a Kolmogorov-Smirnov\n",
    "(KS) test of the hypothesis that those times are a\n",
    "sample of iid uniform random variables\"\n",
    "\n",
    "__2 Sample Kolmogorov-Smirnov (KS) test:__ The Kolmogorov–Smirnov test is a nonparametric test of if two sample distributions are drawn from the same distribution. The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution functions of the samples. \n",
    "\n",
    "<img src=\"./KS2_Example.png\" width=600>\n",
    "> Source: Wikipedia, https://en.wikipedia.org/wiki/Kolmogorov-Smirnov_test\n",
    "\n",
    "\n",
    "We will draw an empirical sample of occurance days from a uniform random distribution, and do a 2-Sample KS test comparing the random occurance days with the observed occurance days of the events in our declustered catalog. We'll use the function `scipy.stats.ks_2samp()` to perform the KS test. This function takes two distributions as input, and returns a D-statistic and p-value. The D-statistic is the distance between the CDFs of the two samples (black arrow in above figure). The closer this number is to 0 the more likely it is that the two samples were drawn from the same distribution. The p-value gives the probability that two samples drawn from the same distribution would have that D-statistic. You can reject the null hypothesis that the two samples were drawn from the same distribution if the p-value is LESS than your significance level (1%, 5%, 10%, etc.). We will repeat this simulation 5000 times, and average the p-value results of the KS-test from all the simlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a KS test on decluster_days and uniformly distributed days\n",
    "num_sims = 5000; #number of simulations\n",
    "# initialize the size of the stats\n",
    "D_ks_7 = np.zeros(num_sims)\n",
    "pvalue_ks_7  = np.zeros(num_sims)\n",
    "D_ks_8 = np.zeros(num_sims)\n",
    "pvalue_ks_8  = np.zeros(num_sims)\n",
    "\n",
    "# perform the simulations\n",
    "for m in np.arange(0,num_sims,1):\n",
    "    iid_days_7=np.random.uniform(0,declustered_days[-1],n_main) # occurance times drawn from uniform dist\n",
    "    ind_d=np.argsort(iid_days_7)   #determine sort index\n",
    "    iid_days_7=iid_days_7[ind_d]     #apply sort index\n",
    "    # KS test for all M>7 events\n",
    "    D_ks_7[m], pvalue_ks_7[m] = scipy.stats.ks_2samp(...,...)\n",
    "    \n",
    "\n",
    "    iid_days_8=np.random.uniform(0,declustered_days[-1],n_8)# occurance times drawn from uniform dist\n",
    "    ind_d=np.argsort(iid_days_8)   #determine sort index\n",
    "    iid_days_8=iid_days_8[ind_d]     #apply sort index\n",
    "    # KS test for M>8 events\n",
    "    D_ks_8[m], pvalue_ks_8[m] =scipy.stats.ks_2samp(...,...)\n",
    "\n",
    "\n",
    "print('p-value for the M≥7 Kolmogorov-Smirnov test: %4.2f percent' %(np.mean(pvalue_ks_7)*100))\n",
    "print('p-value for the M≥8 Kolmogorov-Smirnov test: %4.2f percent' %(np.mean(pvalue_ks_8)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these p-values compare with those published in Shearer and Stark (2012)'s Table 1? Would you reject that the occurance days of our catalog are uniformly distributed (i.e. is the p-value less than say 1% meaning you are 99% confident they are not uniform)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the theoretical (uniform), sample (declustered), and empirical (drawn from uniform) cumulative distributions.\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot([0, n_main],[0, declustered_days[-1]],'-',label='uniform')\n",
    "ax.plot(...,'.',label='drawn from uniform')\n",
    "ax.plot(...,'.',label='declusted')\n",
    "ax.set(xlabel='Number of Events',ylabel='Days',title='M ≥ 7 Events')\n",
    "plt.legend()\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot([0, n_8],[0, declustered_days[-1]],'-',label='uniform')\n",
    "ax.plot(...,'.',label='drawn from uniform')\n",
    "ax.plot(...,'.',label='declusted')\n",
    "ax.set(xlabel='Number of Events',ylabel='Days',title='M ≥ 8 Events')\n",
    "plt.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your KS test p-values corroborate what you observe from the CDFs? Does the M>7 catalog which gave a lower p-value look less uniform than the M>8 catalog?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Global clustering of large earthquakes is not statistically significant:\n",
    "The data are statistically consistent with the hypothesis that\n",
    "these events arise from a homogeneous Poisson process.\"\n",
    "\n",
    "\"Finally, of course,\n",
    "even if the danger has not increased recently, that does not mean\n",
    "that the ongoing danger is small or should be ignored.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
