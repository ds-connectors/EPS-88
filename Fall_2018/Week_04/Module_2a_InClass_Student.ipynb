{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Week 1 In Class Exercise\n",
    "\n",
    "Splitting and filtering data.\n",
    "\n",
    "**Review from Data 8 textbook Chapters **\n",
    "\n",
    "**Before class reading: Fundamentals of Geophysics  **\n",
    "\n",
    "**Last week we:**\n",
    "- Loaded and visualized an earthquake catalog.\n",
    "- Plotted earthquake magnitude and depth.\n",
    "- Learned some more complicated mapping techinque. \n",
    "\n",
    "**Our goals for today:**\n",
    "- pandas DataFrames, indexing, and data cleaning.\n",
    "- Load marine geophysical data (bathymetry and marine magnetic anomalies) from two oceanic ridges.\n",
    "- Select data and drop rows with gaps.\n",
    "- Plot bathymetry data and evaluate spreading rate.\n",
    "- Declare a function to detrend and filter magnetic anomalie data.\n",
    "- Plot marine magnetic anomaly data and compare spreading rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell as it is to setup your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy  as  np\n",
    "import  matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Data Wrangling\n",
    "\n",
    "### Arrays and Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy and pandas offer several types of data structures, the two main structures that we have been using so far and will use in future are `nparray` and `DataFrame`. A `nparray` is a fast and flexible container for large datasets that allows you to perform operations on whole blocks of data at once. Arrays are best suited for homogenous (just one type) numerical data. `DataFrames` are designed for tabular datasets, and can handle heterogenous data (multiple types: int, float, string, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__nparray__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random nparray called arr_data\n",
    "arr_data = np.random.randn(2,3)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .shape to determine the shape of arr_data\n",
    "arr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .dtype to determine the type of arr_data\n",
    "arr_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .ndim to determine the dimensions of arr_data\n",
    "arr_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a nparray of zeros with np.zeros\n",
    "arr0 = np.zeros((4,4))\n",
    "arr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a nparray of ones with np.ones\n",
    "arr1 = np.ones((4,4))\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ones is handy for making a nparray of any single value\n",
    "arr5 = arr1 * 5\n",
    "arr5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of integers between 0 and 10 in steps of 1, including 0 (start) but not 11 (end)\n",
    "arr2 = np.arange(0,11,1) \n",
    "arr2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of floats between 0 and 10 in steps of 2, including 0 (start) but not 11 (end)\n",
    "arr2 = np.arange(0.,11.,2.) \n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of 14 evenly spaced numbers between 0 and 10, including 0 (start) and 10 (end).\n",
    "arr3=np.linspace(0,10,14) \n",
    "arr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DataFrame__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Series` and `DataFrames` are like nparrays but they have the added feature of index labels assigned to each row and column -- the bold labels in the below `DataFrame`. These labels can be used to bin and select data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a new DataFrame\n",
    "# note that index values (like the column labels) don't have to integers and don't have to be in order\n",
    "frame = pd.DataFrame(np.random.rand(3, 3), index=['a','d','c'], columns=['banana','apple','pear'])\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen `DataFrame` structures before in our tabular data files. Such as the .csv (Comma Separated Variable) data file of all the earthquakes of magnitude 4 and higher from 2000 - 2012 in the ANSS (Advanced National Seismic System) Comprehensive Catalog or \"ComCat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog = pd.read_csv('ANSS_2000_2012.csv',header=8,names = ['DateTime','Latitude','Longitude','Depth','Magnitude','MagType','NbStations','Gap','Distance','RMS','Source','EventID'])\n",
    "EQ_catalog.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to reference individual columns (which are called `Series`): `DataFrame.Series` and `DataFrame['Series']`. These do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(EQ_catalog.Depth))\n",
    "print(type(EQ_catalog['Depth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.values` function can be used to return the values of the `Series` as a `nparray`, so without the labled index values of the `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(EQ_catalog.Depth.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/indices.png\" width=900>\n",
    "> Source: Python for Data Analysis (2nd Edition) McKinney, W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/array_slicing.png\" width=900>\n",
    "> Source: Python for Data Analysis (2nd Edition) McKinney, W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random array\n",
    "arr_data = np.random.randn(10,5)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the first 3 rows of arr_data\n",
    "a = arr_data[...]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the last 2 columns of arr_data\n",
    "b = arr_data[..]\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing a `DataFrame` is a bit different because you can reference the index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the first 10 rows of EQ_data\n",
    "EQ_catalog[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice out the a chunk of Depths\n",
    "EQ_catalog.Depth[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just want the values from that chunk and not the index labels use `.values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.Depth.values[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be an issue with elementwise arithmetic because in the `DataFrame` case the original index labels are maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.Depth[5:10]+EQ_catalog.Depth[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.Depth.values[5:10]+EQ_catalog.Depth.values[10:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `reindex` to rearrange or add/delete DataFrame index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2 = frame.reindex(['a','b','c','d'])\n",
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Boolean indexing to filter out values from our DataFrame where the argument we want is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Boolean Indexing to fish out rows with magnitudes larger than 6.5\n",
    "large_mag=EQ_catalog[...]\n",
    "\n",
    "large_mag.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Boolean Indexing to fish out depths of earthqukes with magnitudes larger than 6.5\n",
    "large_mag_depths=xxx[EQ_catalog.Magnitude>=6.5]\n",
    "large_mag_depths.head()\n",
    "\n",
    "# note that the original index information is retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nparrays can be sorted using the `.sort()` method. Put the axis value you want to sort by in the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = np.random.randn(6,3)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data.sort(0)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this sorts all the columns(rows) rather than sorting by just one column(row) and the maintaining rows(columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = np.random.randn(6,3)\n",
    "arr_data.sort(1)\n",
    "arr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be sorted by their index value (`.sort_index`) or by the values in that column (`.sort_values`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_index(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_values(by=['Magnitude']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can reverse the order of sorting with `ascending=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_values(by=['Magnitude'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort by first on column than another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_catalog.sort_values(by=['Magnitude','Depth']).tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - replacing data, removing data, find duplicates and missing data (NaNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.where` can be used to replace values of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_data = np.random.randn(6,3)\n",
    "print(arr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the elements of arr_data that are <0 with 3.0\n",
    "arr_data2=np.where(...) \n",
    "arr_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.drop()` can be used to drop whole columns from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data=EQ_catalog.drop(['MagType','NbStations','Gap', 'Distance','RMS','Source','EventID'], axis='columns')\n",
    "EQ_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.unique()` returns the unique values from the specified object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mags = EQ_data.Magnitude.unique()\n",
    "unique_mags.sort()\n",
    "unique_mags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.value_counts()` returns the count of each unique value from the specified object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data.Magnitude.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to find duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data.DateTime.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EQ_data[EQ_data.DateTime == '2001/03/07 02:49:42.87']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two earthquakes at the same time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.isnan` returns a boolean object with True where NaNs appear in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(frame2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your turn__\n",
    "\n",
    "Create a new 5x3 `DataFrame` of random numbers using `something = pd.DataFrame( ... , index=[''], columns=[''])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort you `DataFrame` by the first column using `.sort_values(by=[''])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the rows where the second column is postive using boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2: Marine Geology - Bathymetry and Magnetic Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at marine magnetics and bathymetry data from two surveys, one from the Mid-Atlantic Ridge and one from the East Pacific Rise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the atlantic data, and then we'll need to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seafloor depth, marine mag anom data\n",
    "# Source: https://maps.ngdc.noaa.gov/viewers/geophysics/\n",
    "#names=['SURVEY_ID','TIMEZONE','DATE','TIME','LAT','LON','POS_TYPE','NAV_QUALCO','BAT_TTIME','CORR_DEPTH','BAT_CPCO','BAT_TYPCO','BAT_QUALCO','MAG_TOT','MAG_TOT2','MAG_RES','MAG_RESSEN','MAG_DICORR','MAG_SDEPTH','MAG_QUALCO','GRA_OBS','EOTVOS','FREEAIR','GRA_QUALCO','LINEID','POINTID'])\n",
    "\n",
    "atlantic_data_file=pd.read_table('data_tracks/vanc05mv.m77t')\n",
    "atlantic_data_file.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `.drop` to remove the columns we won't be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_data_slim = atlantic_data_file.xxx(['SURVEY_ID','TIMEZONE','DATE','TIME','POS_TYPE','NAV_QUALCO','BAT_TTIME','BAT_CPCO','BAT_TYPCO','BAT_QUALCO','MAG_TOT2','MAG_RES','MAG_RESSEN','MAG_DICORR','MAG_SDEPTH','MAG_QUALCO','GRA_OBS','EOTVOS','FREEAIR','GRA_QUALCO','LINEID','POINTID'], axis='columns')\n",
    "\n",
    "atlantic_data_slim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use `np.isnan` to remove rows were we don't have depth AND magnetic field measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_data_clean = atlantic_data_slim[~np.isnan(atlantic_data_slim.CORR_DEPTH) &  ~np.isnan(atlantic_data_slim.MAG_TOT)];\n",
    "atlantic_data_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_data_clean.LON,-1*atlantic_data_clean.CORR_DEPTH,color='mediumblue');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Bathymentry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atlantic_data_clean.LON,atlantic_data_clean.MAG_TOT,color='mediumblue');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just analyze the portion of the survey from around the ridge, so from longitudes -24.0 to 0.0 degrees. So we'll use Boolean indexing to pull out rows of `atlantic_data_clean` where `atlantic_data_clean.LON` is between those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_data = atlantic_data_clean[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_lat=atlantic_data.LAT;\n",
    "atl_lon=atlantic_data.LON;\n",
    "atl_depth=atlantic_data.CORR_DEPTH;\n",
    "atl_total_mag=atlantic_data.MAG_TOT;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a map of where our survey line was collected with a grid of seafloor bathymetry in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/MAR_track_map.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_lon,-1*atl_depth,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Bathymentry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_lon,atl_total_mag,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used another program to project the latitude and longitude coordinates to distance from the ridge along the ship track azimuth -- let's load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_atlantic_data=pd.read_csv('data_tracks/projected_vanc05mv.csv',names=['DIST','DEPTH','MAG_TOT'])\n",
    "atl_dist=projected_atlantic_data.DIST;\n",
    "atl_depth=projected_atlantic_data.DEPTH;\n",
    "atl_total_mag=projected_atlantic_data.MAG_TOT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_dist,-1*atl_depth,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Distance to Ridge, km');\n",
    "plt.ylabel('Bathymentry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(atl_dist,atl_total_mag,color='mediumblue');\n",
    "plt.title('Mid Atlantic Ridge')\n",
    "plt.xlabel('Distance to Ridge, km');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load and clean the data from the East Pacific Rise. This time we'll select date from Longitudes between -126.0 and -95.0 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the seafloor depth, marine mag anom data\n",
    "# Source: https://maps.ngdc.noaa.gov/viewers/geophysics/\n",
    "#names=['SURVEY_ID','TIMEZONE','DATE','TIME','LAT','LON','POS_TYPE','NAV_QUALCO','BAT_TTIME','CORR_DEPTH','BAT_CPCO','BAT_TYPCO','BAT_QUALCO','MAG_TOT','MAG_TOT2','MAG_RES','MAG_RESSEN','MAG_DICORR','MAG_SDEPTH','MAG_QUALCO','GRA_OBS','EOTVOS','FREEAIR','GRA_QUALCO','LINEID','POINTID'])\n",
    "\n",
    "pacific_data_file=pd.read_table('data_tracks/nbp9707.m77t')\n",
    "\n",
    "pacific_data_clean = pacific_data_file[...]; #use ~np.isnan to clear out rows were there are nans\n",
    "pacific_data = pacific_data_clean[...] # use Boolean indexing to select rows with Longitude -126 deg to -95 deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_lat=pacific_data.LAT;\n",
    "pac_lon=pacific_data.LON;\n",
    "pac_depth=pacific_data.CORR_DEPTH;\n",
    "pac_total_mag=pacific_data.MAG_TOT;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a map of where our survey line was collected with a grid of seafloor bathymetry in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/EPR_track_map.png\" width=900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(pac_lon,-1*pac_depth,color='tomato');\n",
    "plt.title('East Pacific Rise');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Bathymetry, m');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,(20,4))\n",
    "plt.plot(pac_lon,pac_total_mag,color='tomato');\n",
    "plt.title('East Pacific Rise');\n",
    "plt.xlabel('Longitude, degrees');\n",
    "plt.ylabel('Total magnetic field, nT');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I used another program to project the latitude and longitude coordinates to distance from the ridge along the ship track azimuth -- let's load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_pacific_data=pd.read_csv('data_tracks/projected_nbp9707.csv',names=['DIST','DEPTH','MAG_TOT'])\n",
    "pac_dist=projected_pacific_data.DIST;\n",
    "pac_depth=projected_pacific_data.DEPTH;\n",
    "pac_total_mag=projected_pacific_data.MAG_TOT;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bathymetry__\n",
    "\n",
    "Now let's compare the two ridges' bathymetry. \n",
    "\n",
    "Let's plot them together on one figure as subplots. First we use `.GridSpec` to set up the grid of subplots, then we use `fig.add_subplot` to set up the subplot axes, and then we can start adding our plot elements to the subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,(20,8)) # create figure object\n",
    "grid = plt.GridSpec(2, 1, wspace=0.4, hspace=0.3) # create grid reference frame and spacing for 2 vertical subplots\n",
    "\n",
    "ax1=fig.add_subplot(grid[0,0]) # create axis object for top subplot\n",
    "ax2=fig.add_subplot(grid[1,0]) # create axis object for bottom subplot\n",
    "\n",
    "ax1.plot(...,...,color='tomato'); # plot the pacific bathymetry\n",
    "ax1.set_xlim(-1000, 1000); # set the x axis range\n",
    "ax1.set_ylim(-5000, -1000); # set the y  axis range\n",
    "ax1.set_xlabel('Distance to Ridge, km'); # labels!\n",
    "ax1.set_ylabel('Bathymetry, m');\n",
    "ax1.set_title('East Pacific Rise');\n",
    "ax1.grid() # add a grid\n",
    "\n",
    "ax2.plot(...,...,color='mediumblue'); # plot the atlantic bathymetry\n",
    "ax2.set_xlim(-1000, 1000);\n",
    "ax2.set_ylim(-5000, -1000);\n",
    "ax2.set_xlabel('Distance to Ridge, km');\n",
    "ax2.set_ylabel('Bathymetry, m');\n",
    "ax2.set_title('Mid Atlantic Ridge');\n",
    "ax2.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/spreading_ridges.png\" width=900>\n",
    "> Source: Essentials of Geology (13th Edition) Lutgens, Tarbuck, and Tasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the bathymetry? Do these ridges have a rift valley at the center? Is the slope steep or gentle? Is the bathymetry rough or smooth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ridge bathymetry, which spreading center do you think is spreading faster the Atlantic (blue) or Pacific (red)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Crustal Magnetic Field__\n",
    "\n",
    "Now we compare the evidence from their marine magnetic field data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,(20,8))\n",
    "grid = plt.GridSpec(2, 1, wspace=0.4, hspace=0.3)\n",
    "\n",
    "ax0=fig.add_subplot(grid[0,0])\n",
    "ax1=fig.add_subplot(grid[1,0])\n",
    "\n",
    "ax0.plot(pac_dist,pac_total_mag,color='tomato');\n",
    "ax0.set_xlim(-1000, 1000);\n",
    "ax0.set_xlabel('Distance to Ridge, km');\n",
    "ax0.set_ylabel('Total Field, nT');\n",
    "ax0.set_title('East Pacific Rise');\n",
    "\n",
    "ax1.plot(atl_dist,atl_total_mag,color='mediumblue');\n",
    "ax1.set_xlim(-1000, 1000);\n",
    "ax1.set_xlabel('Distance to Ridge, km');\n",
    "ax1.set_ylabel('Total Field, nT');\n",
    "ax1.set_title('Mid Atlantic Ridge');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm defining a new function `total2anom` to process these total magnetic field measurements into magnetic anomaly by removing the background drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total2anom(total_mag, distance):\n",
    "    \"\"\"\n",
    "    Simple function (i.e. stupid, doesn't use knowledge of background field from observatory) to process \n",
    "    measured total magnetic field to magnetic anomaly. Detrends and highpass filters the total field.\n",
    "    \n",
    "    inputs: \n",
    "    total magnetic field measurement\n",
    "    distance from the ridge in km\n",
    "    \n",
    "    output:\n",
    "    marine magnetic anomaly (detrended and filtered total field)\n",
    "    \"\"\"\n",
    "    total_detrended = signal.detrend(total_mag); # detrend to remove drift\n",
    "    sample_dist = np.mean(abs(distance.values[1:]-distance.values[0:-1])); # determine sample spacing\n",
    "    fs = 1/sample_dist; # sampling frequency in km^-1\n",
    "    fN = fs *0.5; # Nyquist frequency\n",
    "    # design filter coefficents for highpass filter - 0 to 1/500km filtered, 1/450km to fN passed, \n",
    "    # remove nonlinear drift\n",
    "    filter_coefs = signal.remez(1001, [0, 0.002, 0.00222, fN], [0, 1], Hz=fs);\n",
    "    # apply the filter to the detrended anomaly\n",
    "    filtered_anom = signal.filtfilt(filter_coefs, [1], total_detrended, padlen=len(total_detrended)-1)\n",
    "    \n",
    "    return filtered_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this `total2anom` function to compute the marine magnetic anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atl_mma = total2anom(atl_total_mag,atl_dist)\n",
    "pac_mma = total2anom(pac_total_mag,pac_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1,(20,8))\n",
    "grid = plt.GridSpec(2, 1, wspace=0.4, hspace=0.3)\n",
    "\n",
    "ax0=fig.add_subplot(grid[0,0])\n",
    "ax1=fig.add_subplot(grid[1,0])\n",
    "\n",
    "ax0.plot(pac_dist[:],np.zeros(pac_dist.shape),color='black'); # plot a black reference line at 0 nT\n",
    "ax0.plot(...,...,color='tomato'); # plot the pacific marine magnetic anomaly\n",
    "ax0.set_xlim(-1000, 1000);\n",
    "ax0.set_xlabel('Distance to Ridge, km');\n",
    "ax0.set_ylabel('Magnetic Anomaly, nT');\n",
    "ax0.set_title('East Pacific Rise');\n",
    "\n",
    "ax1.plot(...,...,color='black'); # plot a black reference line at 0 nT\n",
    "ax1.plot(atl_dist,atl_mma,color='mediumblue'); # plot the atlantic marine magnetic anomaly\n",
    "ax1.set_xlim(-1000, 1000);\n",
    "ax1.set_xlabel('Distance to Ridge, km');\n",
    "ax1.set_ylabel('Magnetic Anomaly, nT');\n",
    "ax1.set_title('Mid Atlantic Ridge');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the marine magnetic anomalies together as subplots again with reference lines at zero nT, but zoom in the `xlim` to $\\pm$250 km for the pacific data and $\\pm$150 km for the atlantic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which wiggles can you match between lines and to the model profile due to the GPTS below? Can you pick the Bruhnes, Matuyama, Gauss, and Gilbet polarity chrons? What distance from the ridge does the Bruhnes-Matuyama reversal (which tells us an age of 780 kyr) occur at for both ridges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/marine_mag_anom.png\" width=900>\n",
    "> Source: Fundamentals of Geophysics (2nd Edition) Lowrie, W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the marine magnetic anomalies, which spreading center do you think is spreading faster the Atlantic (blue) or Pacific (red)? Is that consistent with your estimate from the bathymetry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a new DataFrame of our Distance, Depth, Marine Magnetic Anomaly output as a .csv file that we can open again later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame({'Distance':atl_dist, 'Depth':atl_depth, 'Magnetic_Anomaly':atl_mma})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv(\"Atlantic_dist_depth_mma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame({'Distance':pac_dist, 'Depth':pac_depth, 'Magnetic_Anomaly':pac_mma})\n",
    "data2.to_csv(\"Pacific_dist_depth_mma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
